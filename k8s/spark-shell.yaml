apiVersion: v1
kind: Service
metadata:
  name: spark-shell-service
  namespace: spark
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "4040"
    prometheus.io/path: "/metrics/driver"
  labels:
    app: spark-shell
spec:
  selector:
    app: spark-shell
  ports:
    - name: driver-rpc-port
      port: 7077
      targetPort: 7077
    - name: blockmanager
      port: 7078
      targetPort: 7078
    - name: spark-ui
      port: 4040
      targetPort: 4040
  type: ClusterIP

---

apiVersion: v1
kind: Pod
metadata:
  name: spark-shell
  namespace: spark
  labels:
    app: spark-shell
    spark-role: driver
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "4040"
    prometheus.io/path: "/metrics/driver"
spec:
  serviceAccountName: spark-driver
  containers:
    - name: spark-container
      image: vishalpancholi/spark-delta:v15
      command: ["/bin/bash"]
      args: ["-c", "while true; do sleep 30; done"]
      env:
        - name: HIVE_METASTORE_JDBC_URL
          valueFrom:
            secretKeyRef:
              name: credentials
              key: jdbc-url
        - name: HIVE_METASTORE_USER
          valueFrom:
            secretKeyRef:
              name: credentials
              key: username
        - name: HIVE_METASTORE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: credentials
              key: password
        - name: STORAGE_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: credentials
              key: accesskey
        - name: PYSPARK_PYTHON
          value: python3

      ports:
        # named ports required for Spark and Prometheus scraping.
        # This makes the ports discoverable by ServiceMonitors/PodMonitors targeting named ports.
        - name: driver-rpc-port
          containerPort: 7077
        - name: blockmanager
          containerPort: 7078
        - name: spark-ui
          containerPort: 4040
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "2Gi"
          cpu: "1"
      volumeMounts:
        - name: spark-config-volume
          mountPath: /opt/spark/conf/spark-defaults.conf
          subPath: spark-defaults.conf
  volumes:
    - name: spark-config-volume
      configMap:
        name: spark-config
  restartPolicy: Always


